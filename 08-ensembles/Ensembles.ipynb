{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HSE-LAMBDA/MLDM-2022/blob/main/08-ensembles/Ensembles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwaWHN8HxBi5"
   },
   "source": [
    "# Ensembles: bagging & boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IM02fOjxcMic"
   },
   "source": [
    "### Preliminary things (from the last seminar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EBDdjJ-ay0M"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBmN8Ildbe0d"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBVDpZ6Y-jB5"
   },
   "source": [
    "Here's a function that makes a 2d decision boundary plot for a given classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGp3PLfW35hF"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def plot_decision_surface(\n",
    "                  clf, X, y,\n",
    "                  nx=200, ny=100,\n",
    "                  cmap='bwr',\n",
    "                  alpha=0.6,\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary of clf on X and y, visualize training points\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the grid\n",
    "    x_top_left = X.min(axis=0) - 1\n",
    "    x_bottom_right = X.max(axis=0) + 1\n",
    "    grid_x0, grid_x1 = np.meshgrid(\n",
    "         np.linspace(x_top_left[0], x_bottom_right[0], ny),\n",
    "         np.linspace(x_top_left[1], x_bottom_right[1], nx)\n",
    "      )\n",
    "    \n",
    "    # Calculate predictions on the grid\n",
    "    y_pred_grid = clf.predict(\n",
    "                        np.stack(\n",
    "                              [\n",
    "                                grid_x0.ravel(),\n",
    "                                grid_x1.ravel()\n",
    "                              ],\n",
    "                              axis=1\n",
    "                            )\n",
    "                      ).reshape(grid_x1.shape)\n",
    "    \n",
    "    # Find optimal contour levels and make a filled\n",
    "    # contour plot of predictions\n",
    "    labels = np.sort(np.unique(y))\n",
    "    labels = np.concatenate([[labels[0] - 1],\n",
    "                             labels,\n",
    "                             [labels[-1] + 1]])\n",
    "    medians = (labels[1:] + labels[:-1]) / 2\n",
    "    plt.contourf(grid_x0, grid_x1, y_pred_grid, cmap=cmap, alpha=alpha,\n",
    "                 levels=medians)\n",
    "    \n",
    "    # Scatter data points on top of the plot,\n",
    "    # with different styles for correct and wrong\n",
    "    # predictions\n",
    "    y_pred = clf.predict(X)\n",
    "    plt.scatter(*X[y_pred==y].T, c=y[y_pred==y],\n",
    "                marker='o', cmap=cmap, s=10, label='correct')\n",
    "    plt.scatter(*X[y_pred!=y].T, c=y[y_pred!=y],\n",
    "                marker='x', cmap=cmap, s=50, label='errors')\n",
    "\n",
    "    # Dummy plot call to print the accuracy in the legend.\n",
    "    plt.plot([], [], ' ',\n",
    "             label='Accuracy = {:.3f}'.format(accuracy_score(y, y_pred)))\n",
    "    \n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lgCWhbEc77C"
   },
   "source": [
    "#### Toy multiclass data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wahVk0jegnb_"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/HSE-LAMBDA/MLDM-2022/raw/master/07-trees/data.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7iUjzyo-jCU"
   },
   "source": [
    "Firstly, we'll load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2qKojahYhm1"
   },
   "outputs": [],
   "source": [
    "data = np.load('data.npz')\n",
    "X, y = data[\"X\"], data[\"y\"]\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5l3oJDs-jCi"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(*X.T, c=y, cmap='rainbow');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cuuN5L6-jCb"
   },
   "source": [
    "And then split it to train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybPi8TNtYlgV"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.5, random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6NA1BGH-jCo"
   },
   "source": [
    "Now that we've had a look at the data, let's fit a decision tree on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dvkVwtLZjDA"
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(min_samples_leaf=12)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3s03yi0-jCs"
   },
   "source": [
    "and plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEHFL51JY02v"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 16))\n",
    "plt.subplot(2, 1, 1)\n",
    "plot_decision_surface(clf, X_train, y_train, cmap='rainbow')\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_decision_surface(clf, X_test, y_test, cmap='rainbow');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtKtjwBDdKZA"
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmU84QKFs1qx"
   },
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExxGyP0b7s9c"
   },
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8S1A9-L4tAjo"
   },
   "source": [
    "Let's build our own decision tree bagging and see how well it works. Implement the **`BagOfTrees`** class below **(2 points)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQhRF4kChB18"
   },
   "outputs": [],
   "source": [
    "class BagOfTrees:\n",
    "  def __init__(self, n_estimators=10, **kwargs):\n",
    "    self.trees = []\n",
    "    for i in range(n_estimators):\n",
    "        self.trees.append(DecisionTreeClassifier(**kwargs))\n",
    "        \n",
    "  def fit(self, X, y):   \n",
    "    # Fit each of the trees on a random subset of X and y.\n",
    "    # hint: you can select random subsample of data like this:\n",
    "    # >>> ix = np.random.randint(0, len(X), len(X))\n",
    "    # >>> X_sample, y_sample = X[ix], y[ix]\n",
    "\n",
    "    <YOUR CODE>\n",
    "    \n",
    "  def predict(self, X):\n",
    "    trees = self.trees\n",
    "    \n",
    "    # Compute predictions of each tree and aggregate them into\n",
    "    # the ensemble prediction\n",
    "    # Note: you can use tree.predict(X) to get the predicted classes\n",
    "    # or tree.predict_proba(X) to get individual probabilities\n",
    "    # for all classes\n",
    "    \n",
    "    return <np.array of prediction indices>\n",
    "  \n",
    "# once you think you're done, see if your code passes the asserts below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMwTGrE6xQxY"
   },
   "outputs": [],
   "source": [
    "model = BagOfTrees(n_estimators=100, min_samples_leaf=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_test[::100])\n",
    "print(\"predictions:\", pred)\n",
    "assert isinstance(pred, np.ndarray), \"prediction must be a numpy array\"\n",
    "assert str(pred.dtype).startswith('int'), \"prediction dtype must be integer (int32/int64)\"\n",
    "assert pred.ndim == 1, \"prediction must be a vector (1-dimensional)\"\n",
    "assert len(pred) == len(X_test[::100]), \"must predict exactly one answer for each input (expected length %i, got %i)\" % (len(X_test[::100]), len(pred))\n",
    "assert any(model.trees[0].predict(X_train) != model.trees[1].predict(X_train)), \"Some trees are identical. Did you forget to train each tree on a random part of the data?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRqkgnB_-jDH"
   },
   "source": [
    "If the cell above executes without errors, run the code below to compare overall accuracy with individual tree accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuVXG232hBuy"
   },
   "outputs": [],
   "source": [
    "for i, tree in enumerate(model.trees[:5]):\n",
    "    print(\"tree {} individual accuracy = {:.5f}\".format(\n",
    "        i, accuracy_score(y_test, tree.predict(X_test))\n",
    "      ))\n",
    "\n",
    "print(\"Ensemble accuracy:\",\n",
    "      accuracy_score(model.predict(X_test), y_test)) # should be >= 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nh5htnXF-jDM"
   },
   "source": [
    "And have a look at the decision surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1X3TZ1iMx3oO"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 16))\n",
    "plt.subplot(2, 1, 1)\n",
    "plot_decision_surface(model, X_train, y_train, cmap='rainbow')\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_decision_surface(model, X_test, y_test, cmap='rainbow');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3wX9mTNCWUn"
   },
   "source": [
    "Now let's check how train and test accuracy depends on the number of estimators **(1 point)**.\n",
    "\n",
    "Hint: instead of fitting a new BagOfTrees for each number of estimators we can just fit the maximum number and then iteratively predict and remove the fitted trees one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yx5pt9QMDRyZ"
   },
   "outputs": [],
   "source": [
    "model = BagOfTrees(n_estimators=100, min_samples_split=30, splitter='random', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "n_trees = []\n",
    "test_score = []\n",
    "train_score = []\n",
    "\n",
    "<YOUR CODE HERE> # fill the lists above to make the plot\n",
    "\n",
    "\n",
    "plt.plot(n_trees, train_score, label='train')\n",
    "plt.plot(n_trees, test_score, label='test')\n",
    "plt.legend()\n",
    "plt.xlabel('number of trees')\n",
    "plt.ylabel('accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkW93y5C0Mqw"
   },
   "source": [
    "```\n",
    "```\n",
    "```\n",
    "```\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mzvrpP_0UnQ"
   },
   "source": [
    "### Pre-implemented ensembles: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECeEe3MH05BB"
   },
   "source": [
    "RandomForest combines bagging and random subspaces: each tree uses a fraction of training samples, and the splits are chosen among subsets of features. Typically this leads to a slightly better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGns4GcZx3kM"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Task: create and fit a random forest with\n",
    "# 100 estimators and at least 5 samples per leaf\n",
    "\n",
    "model = <YOUR CODE>\n",
    "\n",
    "<YOUR CODE>\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_decision_surface(model, X_test, y_test, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfDOLLV22djx"
   },
   "source": [
    "```\n",
    "```\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtM1TZql2fCP"
   },
   "source": [
    "### Pre-implemented ensembles: Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXbycsrJYQTW"
   },
   "source": [
    "One of the most commonly used libraries for gradient boosing is the [XGBoost library](https://xgboost.ai/). Consider reading [this document](https://xgboost.readthedocs.io/en/latest/tutorials/model.html) for an introduction to the algorithm.\n",
    "\n",
    "Here's the [help page](https://xgboost.readthedocs.io/en/latest/parameter.html) listing available parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zamTOiyMZNHD"
   },
   "source": [
    "Let's start by importing the classifier class and the function that plots individual trees as graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocW-MLi90LxD"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier, plot_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eK3C_XPjZtfU"
   },
   "source": [
    "We can now investigate how decision surface depends on the number of trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ny94kTDqPzPO"
   },
   "outputs": [],
   "source": [
    "for n_estimators in range(1,10):\n",
    "    model = <YOUR CODE> # create an XGBClassifier with trees of depth 1,\n",
    "                        # learning rate 0.5 and n_estimators estimators\n",
    "\n",
    "    <YOUR CODE> # fit this model to the train data\n",
    "\n",
    "    print(\"n_estimators = \", n_estimators)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plot_decision_surface(model, X_test, y_test, cmap='rainbow')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paFmsQONaQfc"
   },
   "source": [
    "And here's how one may use the `plot_tree` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pPp5qkVnXJbC"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "plot_tree(model, num_trees=44, ax=ax, dpi='400');\n",
    "#                   ^^^ This parameter selects the\n",
    "#                       tree that you want to plot.\n",
    "#                       Since there's 9 estimators\n",
    "#                       in the last model and 5\n",
    "#                       classes in our data, the total\n",
    "#                       amount of individual trees\n",
    "#                       is 45 (from 0 to 44)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSu9dBVnvawb"
   },
   "source": [
    "<font color='red'>**Warning:**</font> current xgboost implementation is not very safe to typos, i.e. it can silently swallow whatever argument you provide, even if it has no effect, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoh-aaQlv5-R"
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier(abrakadabra=\"I won't change anything\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rq_GvLOgwMHW"
   },
   "source": [
    "so be sure to check your spelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErviY92bwdBE"
   },
   "source": [
    "Now let's try to improve the score by adjusting the parameters. Here are some of the parameters you may want to try:\n",
    "  - `max_depth` – maximum tree depth,\n",
    "  - `n_estimators` – number of trees (per class),\n",
    "  - `learning_rate` – shrinkage,\n",
    "  - `reg_lambda` – L2 regularization term on weights,\n",
    "  - `subsample` – row random subsampling rate (per tree),\n",
    "  - `colsample_bynode` – column subsampling rate (per node)\n",
    "  - `gamma` – minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "\n",
    "See [this page](https://xgboost.readthedocs.io/en/latest/parameter.html) for more information.\n",
    "\n",
    "  > *Hint: since XGBClassifier has the same interface as sklearn models, you can use GridSearchCV on it if you want.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTt9TgEIwjrZ"
   },
   "outputs": [],
   "source": [
    "model = <YOUR CODE HERE>\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plot_decision_surface(model, X_test, y_test, cmap='rainbow')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJ-CUSQxfaxB"
   },
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pB1EQwH5mVkm"
   },
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OT08rKZMftQF"
   },
   "source": [
    "[Optuna](https://optuna.org/) is framework agnostic. You can use it with any machine learning or deep learning framework like sklearn, catboost, PyTorch, etc.\n",
    "\n",
    "You can optimize XGBoost hyperparameters in three steps:\n",
    "\n",
    "1. Wrap model training with an `objective` function and return accuracy\n",
    "2. Suggest hyperparameters using a `trial` object\n",
    "3. Create a `study` object and execute the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epl49ZgRfXbI"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # define the grid\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    bst = XGBClassifier()\n",
    "    bst.set_params(**param)\n",
    "    bst.fit(X_train, y_train)\n",
    "    preds = bst.predict(X_test)\n",
    "    pred_labels = np.rint(preds)\n",
    "    # objective should return the metrics that you want to optimize\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_test, pred_labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmXr62SQg-IQ"
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "# you may increase the nubmer of trials in case you have enough time\n",
    "study.optimize(objective, n_trials=10, timeout=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2J-bsMOhTuj"
   },
   "outputs": [],
   "source": [
    "print(\"  Best value: {}\".format(study.best_trial.value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yz6UN7LnjyZ7"
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "model.set_params(**study.best_params)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plot_decision_surface(model, X_test, y_test, cmap='rainbow')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBpPvhuS3Xa0"
   },
   "source": [
    "```\n",
    "```\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WehLBvvh3gQb"
   },
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPGAIyyF-Nli"
   },
   "source": [
    "For this example we are going to use California Housing Dataset.\n",
    "\n",
    "The **target** variable is the median house value for California districts.\n",
    "\n",
    "\n",
    "The features are:\n",
    " *   **MedInc** median income in block\n",
    " *   **HouseAge** median house age in block\n",
    " *   **AveRooms** average number of rooms\n",
    " *   **AveBedrms** average number of bedrooms\n",
    " *   **Population** block population\n",
    " *   **AveOccup** average house occupancy\n",
    " *   **Latitude** house block latitude\n",
    " *   **Longitude** house block longitude\n",
    " \n",
    " More information [here](https://scikit-learn.org/stable/datasets/index.html#california-housing-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAIX2GlA3o1Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVuypXO13vjy"
   },
   "outputs": [],
   "source": [
    "dataset = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcAKa7Uy-dnB"
   },
   "source": [
    "`dataset` holds the data in numpy arrays, but we can convert it to a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wM50xJNq4gbE"
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "data['target'] = dataset.target\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMCRUqM9-s_H"
   },
   "source": [
    "Now, let's separate the features from the target and split the data to train and test parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hoxs850jQuT8"
   },
   "outputs": [],
   "source": [
    "data_X = data.drop('target', axis=1)\n",
    "data_y = data['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size=0.5)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYurIqUG-4Fj"
   },
   "source": [
    "and grid search for best random forest parameters on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-UXdggjT7VL"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=30)\n",
    "\n",
    "gscv = GridSearchCV(model,\n",
    "                    param_grid=<YOUR CODE HERE>,\n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    n_jobs=-1,\n",
    "                    cv=3,\n",
    "                    verbose=3)\n",
    "\n",
    "\n",
    "gscv.fit(X_train, y_train)\n",
    "model = gscv.best_estimator_\n",
    "print(model)\n",
    "\n",
    "print(\"Train loss:\", mean_squared_error(y_train, model.predict(X_train)))\n",
    "print(\"Test loss:\" , mean_squared_error(y_test , model.predict(X_test )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCyXJiJl-_Ny"
   },
   "source": [
    "Once fit, the model has the information about importances of individual features, calculated from gain in individual splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOGUyPDk7XU5"
   },
   "outputs": [],
   "source": [
    "# get the estimates of feature importances\n",
    "importances = model.feature_importances_\n",
    "# calculate the std:\n",
    "std = np.std([tree.feature_importances_ for tree in model.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# This part will be used to make nice x-axis labels\n",
    "# (we'll tell matplotlib to convert numeric feature\n",
    "# index to a text label):\n",
    "from matplotlib.ticker import FuncFormatter, MaxNLocator\n",
    "def format_fn(tick_val, tick_pos):\n",
    "    if int(tick_val) in range(len(importances)):\n",
    "        return dataset.feature_names[int(tick_val)]\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13,7))\n",
    "ax.set_title(\"Feature importances\")\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(format_fn))\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.bar(range(len(importances)), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "ax.set_xlim([-1, len(importances)])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmE4OCv8mJU3"
   },
   "source": [
    "You may also use trees to evaluate the impurity based importance of the pixels in an image classification task on the faces dataset. The hotter the pixel, the more important it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enIZksFUmJ-P"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeYBqHFOmLfM"
   },
   "source": [
    "This dataset contains a set of face images taken between April 1992 and April 1994 at AT&T Laboratories Cambridge. \n",
    "\n",
    "There are ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement).\n",
    "\n",
    "The “target” for this database is an integer from 0 to 39 indicating the identity of the person pictured; however, with only 10 examples per class, this is relatively small dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpj_T96CmNxq"
   },
   "outputs": [],
   "source": [
    "data = fetch_olivetti_faces()\n",
    "X, y = data.data, data.target\n",
    "mask = y < 5 # let's pick only 5 classes\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "model = RandomForestClassifier(n_estimators=750, random_state=42)\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlKwwA4dmSiS"
   },
   "outputs": [],
   "source": [
    "img_shape = data.images[0].shape\n",
    "importances = model.feature_importances_\n",
    "imp_reshaped = importances.reshape(img_shape)\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.matshow(imp_reshaped, fignum=1)\n",
    "plt.title(\"Pixel importances using impurity values\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
