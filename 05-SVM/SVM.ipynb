{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HSE-LAMBDA/MLDM-2022/blob/master/05-SVM/SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyMZFJaS2P-B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OWWP7w72P-G"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.datasets import make_circles, make_classification, make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "from matplotlib import colors\n",
        "cmap = colors.LinearSegmentedColormap('red_blue_classes',\n",
        "                                      {'red': [(0, 1, 1), (1, 0.7, 0.7)],\n",
        "                                       'green': [(0, 0.7, 0.7), (1, 0.7, 0.7)],\n",
        "                                       'blue': [(0, 0.7, 0.7), (1, 1, 1)]})\n",
        "cm_bright = ListedColormap(['#FF0000', '#0000FF'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiGjaxPQ2P-L"
      },
      "source": [
        "## Linear SVM reminder\n",
        "\n",
        "Let's look at binary classification problem. Training samples are given by $\\{(x_n, y_n)\\}_{n=1}^N$, where $N$ — number of objects, $\\boldsymbol x_n \\in \\mathbb{R}^d$ — feature vector of object, $y_n \\in \\{+1, -1\\}$ — class of object.\n",
        "\n",
        "SVM trains model for separating hyperplane:\n",
        "$$f(\\boldsymbol x) = \\boldsymbol w^T \\boldsymbol x + b$$\n",
        "Parameters of model — vector of weights $\\boldsymbol w \\in \\mathbb{R}^d$ and bias $b \\in \\mathbb{R}$.\n",
        "\n",
        "Training is done by solving optimisation problem:\n",
        "$$\n",
        "\\begin{gather}\n",
        "    \\frac{1}{2} \\| \\boldsymbol w \\|^2 + C \\sum_{n=1}^N \\xi_n \\to \\min_{\\boldsymbol w, \\boldsymbol \\xi, b} \\\\\n",
        "    \\text{s.t.: } \\quad y_n (\\boldsymbol w^T \\boldsymbol x_n + b) \\geq 1 - \\xi_n, \\quad \\xi_n \\geq 0, \\quad \\forall n=1,\\dots,N\n",
        "\\end{gather}\n",
        "$$\n",
        "\n",
        "Constraint $y_n (\\boldsymbol w^T \\boldsymbol x_n + b) \\geq 1$ assures that objects are correctly classified by separating hyperplane. Since in practice the sample could not be linearly separable the slack variables $\\xi_n$ are introduced , which weakens condition of right classification. $\\| \\boldsymbol w \\|^2$ penalise small width of margin,  $\\sum_n \\xi_n$ penalise weakens of constraints. \n",
        "\n",
        "The solution of optimisation problem is given by $(\\boldsymbol w_{\\star}, \\boldsymbol \\xi_{\\star}, b_{\\star})$, some of the constraints become active, i.e. become a exact equality:\n",
        "$$\\quad y_n (\\boldsymbol w_{\\star}^T \\boldsymbol x_n + b_{\\star}) = 1 - \\xi_{\\star,n}$$\n",
        "Objects, corresponding to active constraints called $\\textbf{support vectors}$.\n",
        "\n",
        "\n",
        "Hyperparameter $C$ is responsible of balancing the width of margin and errors, made by classifier. It shows the generalizing property of the separating hyperplane - big values of $C$ corresponds to less generalizing ability and can lead to overfitting, if the data is well described by linear model. To select $C$ one must do cross-validation on validation set to find the best value.\n",
        "\n",
        "### Realisation\n",
        "\n",
        "There are two realisations of linear SVM in sklearn : [LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) and [SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) with *linear* kernel. They build on different libraries, with solve optimisation problem *liblinear* in first case and *libsvm* in second.\n",
        "\n",
        "Here we will use [sklearn.svm.SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) with *kernel='linear'*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWZcj-eU2P-L"
      },
      "source": [
        "We generate data samples with:\n",
        "- linearly separable classes\n",
        "- with well separable classes, but not linearly\n",
        "- with non separable classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZQxR0ET2P-M"
      },
      "source": [
        "### Plotting function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMUt7IW65IuF"
      },
      "outputs": [],
      "source": [
        "def fit_and_plot(X, y, model, Nx=200, Ny=200):\n",
        "  # Splitting the dataset and fitting on the train part\n",
        "  X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(X, y, test_size=.4, random_state=42)\n",
        "\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  # Plotting the `|margin| < 1` band\n",
        "  grid_x0 = np.linspace(X[:,0].min() - 0.5, X[:,0].max() + 0.5, Nx)\n",
        "  grid_x1 = np.linspace(X[:,1].min() - 0.5, X[:,1].max() + 0.5, Ny)\n",
        "\n",
        "  xx0, xx1 = np.meshgrid(grid_x0, grid_x1)\n",
        "  zz = model.decision_function(\n",
        "      np.c_[xx0.ravel(), xx1.ravel()]\n",
        "  ).reshape(xx0.shape)\n",
        "\n",
        "  plt.contourf(xx0, xx1, zz, cmap=plt.cm.RdBu, alpha=.8, linestyles=['--', '-', '--'], levels=[-1, 0, 1])\n",
        "  plt.contour(xx0, xx1, zz, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'], levels=[-1, 0, 1])\n",
        "  plt.pcolormesh(xx0, xx1, zz, cmap=cmap,norm=colors.Normalize(0., 1.), zorder=0)\n",
        "\n",
        "  # Highlighting support vectors\n",
        "  if hasattr(model, \"named_steps\"):\n",
        "    sv = model[:-1].inverse_transform(\n",
        "        model.named_steps['svc'].support_vectors_\n",
        "    )\n",
        "  else:\n",
        "    sv = model.support_vectors_\n",
        "  plt.scatter(*sv.T, s=180, facecolors='none', zorder=10, edgecolors='black', linewidths=0.5)\n",
        "\n",
        "  # Plotting the data points\n",
        "  plt.scatter(*X_train.T, c=y_train, cmap=cm_bright, alpha=0.3, s=36)\n",
        "  plt.scatter(*X_test.T, c=y_test, cmap=cm_bright, alpha=0.8, s=36, marker='<')\n",
        "\n",
        "  # Tweaking the plot a bit\n",
        "  plt.xlim(xx0.min(), xx0.max())\n",
        "  plt.ylim(xx1.min(), xx1.max())\n",
        "  plt.xlabel(\"x0\")\n",
        "  plt.ylabel(\"x1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4FX5517Ecx-"
      },
      "source": [
        "### Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqvhhCxv9lQ7"
      },
      "outputs": [],
      "source": [
        "datasets = []\n",
        "\n",
        "# Linearly separable datset:\n",
        "datasets.append(\n",
        "    make_blobs(n_samples=100, n_features=2, centers=2, cluster_std=1.5, random_state=42)\n",
        ")\n",
        "\n",
        "# Linearly inseparable:\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2,\n",
        "                               random_state=231, n_clusters_per_class=1)\n",
        "rng = np.random.RandomState(2)\n",
        "X += 15 * rng.uniform(size=X.shape)\n",
        "datasets.append((X, y))\n",
        "\n",
        "# Circles:\n",
        "datasets.append(make_circles(n_samples=100, noise=0.05, random_state=42))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k18ULei0Egho"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CWJ0cNEDOgK"
      },
      "outputs": [],
      "source": [
        "X, y = datasets[0] # Try datasets 1 and 2 as well\n",
        "\n",
        "model = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    SVC(C=100., kernel='linear') # Play around with the value of C\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "fit_and_plot(X, y, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Kqhrb50H-Xw"
      },
      "source": [
        "Let's add PolynomialFeatures transformer to the example above.\n",
        "\n",
        "Note: sklearn's PolynomialFeatures transformer doesn't have an `inverse_transform` method defined, so our plotting code won't work. A small hack to fix this:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = datasets[0]\n",
        "\n",
        "poly = PolynomialFeatures(2, include_bias=False)\n",
        "poly.inverse_transform = lambda X: X[:,:2]\n",
        "\n",
        "model = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    poly, \n",
        "    SVC(C=1000., kernel='linear')\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "fit_and_plot(X, y, model)"
      ],
      "metadata": {
        "id": "EPzIDjtAJo4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAlibfBs2P-T"
      },
      "source": [
        "### Number of support vectors\n",
        "\n",
        "How does the number of support vectors depend on C on different datasets?\n",
        "Extract info about support vectors from SVC and plot a graph, representing Number of Support Vectors VS value of C for all 3 datasets **(2 points)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8-3EQWL2P-f"
      },
      "outputs": [],
      "source": [
        "C_values = np.logspace(-4, 4, 50, base = 10)\n",
        "\n",
        "for dataset, labels in zip(datasets, ['sep', 'insep', 'circles']):\n",
        "  # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPpzVH6d2P-r"
      },
      "source": [
        "## Kernel SVM\n",
        "\n",
        "![](http://i.imgur.com/bJAzRCt.png)\n",
        "\n",
        "Linaer SVM problem, covered above, is usually called direct optimisation problem. Any direct problem has $\\textbf{dual}$ problem and in some cases optimums of both problems coincide.\n",
        "\n",
        "\n",
        "Dual problem for SVM is:\n",
        "$$\n",
        "\\begin{gather}\n",
        "    \\sum_{n} \\alpha_n - \\frac{1}{2}\\sum_{n}\\sum_{n'} \\alpha_{n}\\alpha_{n'} y_{n}y_{n'} x_{n}^Tx_{n'} \\to \\max_{\\alpha} \\\\\n",
        "    \\begin{aligned}\n",
        "        \\text{s.t. } \\quad  \n",
        "        & 0 \\le \\alpha_n \\le C, \\quad \\forall n = 1, \\dots, N \\\\\n",
        "        & \\sum_{n} \\alpha_n y_n = 0\n",
        "    \\end{aligned}\n",
        "\\end{gather}\n",
        "$$\n",
        "\n",
        "Vector of dual variables is being optimised $\\alpha_n$. Object $x_n$ is a SV, if $\\alpha_n > 0$.\n",
        "\n",
        "The predicted label is given by:\n",
        "$$\\hat{y}(x) = \\text{sign}\\left(\\sum_{n}\\alpha_{n}y_{n}x^Tx_{n} + b\\right).$$\n",
        "\n",
        "#### Kernel trick\n",
        "Notice, that dual problem has features only as a scalar product $x^Tx'$. This observation helps us to perform kernel trick - implicitly change feature space. Instead of calculating $\\phi(\\boldsymbol x)$ (as we did before) we will compute scalar product $k(\\boldsymbol x, \\boldsymbol x')$ called $\\textbf{kernel}$ and plug it insted of $x^Tx'$ above. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XN4NVvO2P-r"
      },
      "source": [
        "Try different SVM kernels and plot pictures as above, look what other parameters kernels have and how they affect results.\n",
        "- polynomial: $k(x, x') = (\\gamma x^Tx' + r)^d$ with different $d = 2,3,\\dots$\n",
        "- Gaussian RBF: $k(x, x') = \\exp(-\\gamma\\|x - x'\\|^2)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tbldw45VKVTp"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}