{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HSE-LAMBDA/MLDM-2022/blob/master/04-regularization/Regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyNyFPNLVxuA"
      },
      "source": [
        "# Boston housing dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAiD9r1mV5ka"
      },
      "source": [
        "In this example we'll try to predict housing prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIxJDCLGXJ6W"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_boston\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20pkSnXwX5fW"
      },
      "outputs": [],
      "source": [
        "data = load_boston()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yuHygsmWAxW"
      },
      "source": [
        "Information about the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRdA6R3YYH8U"
      },
      "outputs": [],
      "source": [
        "print(data.DESCR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kYvpf7WWRaY"
      },
      "source": [
        "# Exploring the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDmkDPRnWfLh"
      },
      "source": [
        "Let's just see how the target depends on individual features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obHi592HaS7r"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "\n",
        "grid_size = int(np.ceil(X.shape[1]**0.5))\n",
        "\n",
        "for i, (name, x) in enumerate(X.iteritems(), 1):\n",
        "  plt.subplot(grid_size, grid_size, i)\n",
        "  plt.scatter(x, data.target)\n",
        "  plt.xlabel(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11Dameq1X4rI"
      },
      "source": [
        "Let's start by trying a simple linear regression model on the features with the most obvious correlation with the target. We'll also scale the features manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNlSiXP_Ywjg"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KJJ8hzOiY85"
      },
      "outputs": [],
      "source": [
        "columns = [\"CRIM\", \"RM\", \"LSTAT\"]\n",
        "\n",
        "X_subset = X[columns]\n",
        "X_subset /= X_subset.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1Ws8w5-Zw82"
      },
      "outputs": [],
      "source": [
        "model = make_pipeline(\n",
        "    PolynomialFeatures(9, include_bias=False), # Can you calculate how many features this will result in? :)\n",
        "    LinearRegression()\n",
        ")\n",
        "\n",
        "model.fit(X_subset, y)\n",
        "print('mse = ', mean_squared_error(y, model.predict(X_subset)))\n",
        "\n",
        "plt.figure(figsize=(15, 4))\n",
        "for i, c in enumerate(columns, 1):\n",
        "  plt.subplot(1, len(columns), i)\n",
        "  plt.scatter(X[c], y, label='data')\n",
        "  plt.scatter(X[c], model.predict(X_subset), label='prediction')\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4KzosykZHPD"
      },
      "source": [
        "# Splitting the data to train and validation parts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDABmqhgZRJ8"
      },
      "source": [
        "Looks like the fit from above is reasonable, right?\n",
        "\n",
        "In fact, we cannot know this yet: we fitted and predicted on the same data. Let's split our dataset to get a more reasonable estimate of the prediction error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFTt4UrtfW8K"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1yoR0D2fZcM"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_subset, y, test_size=50, random_state=39)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2vyMKoafgdd"
      },
      "outputs": [],
      "source": [
        "model = make_pipeline(\n",
        "    PolynomialFeatures(9, include_bias=False),\n",
        "    LinearRegression()\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print('train mse = ', mean_squared_error(y_train, model.predict(X_train)))\n",
        "print('test mse = ', mean_squared_error(y_test, model.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPdxrrPWZzA1"
      },
      "source": [
        "That's quite an error we have on the test set!\n",
        "\n",
        "Let's look at the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCIVdnmhkDUl"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 4))\n",
        "for i, c in enumerate(columns, 1):\n",
        "  plt.subplot(1, len(columns), i)\n",
        "  plt.scatter(X_test[c], y_test, label='data')\n",
        "  plt.scatter(X_test[c], model.predict(X_test), label='prediction')\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JblHhLeKaq4y"
      },
      "source": [
        "That's because our parameter values at the solution are enormous:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2hRXCvqaYhw"
      },
      "outputs": [],
      "source": [
        "model.named_steps['linearregression'].coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47w2yMDTa3jw"
      },
      "source": [
        "# L2 regularization (ridge regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFqxu7Cia9PG"
      },
      "source": [
        "Let's regularize the solution!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJd9IDwkf0b_"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mPbo3d8gG-w"
      },
      "outputs": [],
      "source": [
        "model = make_pipeline(\n",
        "    PolynomialFeatures(9, include_bias=False),\n",
        "    Ridge(alpha=1.)\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print('train mse = ', mean_squared_error(y_train, model.predict(X_train)))\n",
        "print('test mse = ', mean_squared_error(y_test, model.predict(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGKHmwLNk2wU"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 4))\n",
        "for i, c in enumerate(columns, 1):\n",
        "  plt.subplot(1, len(columns), i)\n",
        "  plt.scatter(X_test[c], y_test, label='data')\n",
        "  plt.scatter(X_test[c], model.predict(X_test), label='prediction')\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxmdOpYJbN3V"
      },
      "outputs": [],
      "source": [
        "model.named_steps['ridge'].coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xHvB9VIbxrP"
      },
      "source": [
        "Now we'll study how losses and parameter values depend on the regularization power."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLn0yQe2nwOw"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjQwZ0g4jSkq"
      },
      "outputs": [],
      "source": [
        "reg_powers = np.logspace(-12, 5, 18 * 5, base=10)\n",
        "\n",
        "\n",
        "train_mse = []\n",
        "test_mse = []\n",
        "\n",
        "params = []\n",
        "\n",
        "for alpha in tqdm(reg_powers):\n",
        "  linear_model = Ridge(alpha=alpha)\n",
        "  model = make_pipeline(\n",
        "    PolynomialFeatures(9, include_bias=False),\n",
        "    linear_model\n",
        "  )\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  params.append(\n",
        "      np.append(linear_model.coef_,\n",
        "                linear_model.intercept_)\n",
        "  )\n",
        "\n",
        "  train_mse.append(mean_squared_error(y_train, model.predict(X_train)))\n",
        "  test_mse.append(mean_squared_error(y_test, model.predict(X_test)))\n",
        "\n",
        "params = np.array(params)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "plt.plot(1. / reg_powers, train_mse, label='train')\n",
        "plt.plot(1. / reg_powers, test_mse, label='test')\n",
        "plt.ylabel('MSE')\n",
        "plt.xlabel('inverse regularization power')\n",
        "plt.legend()\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(1. / reg_powers, np.abs(params));\n",
        "plt.xlabel(\"inverse regularization power\")\n",
        "plt.ylabel(\"parameter magnitude\")\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f1cCilVcouK"
      },
      "source": [
        "# L1 regularization (lasso regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S0rCyOBcvou"
      },
      "source": [
        "Here's a similar study with the Lasso regression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JAYVacvm-CT"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eX6mAO5_oIvR"
      },
      "outputs": [],
      "source": [
        "reg_powers = np.logspace(-4, 1, 6 * 5, base=10)\n",
        "\n",
        "train_mse = []\n",
        "test_mse = []\n",
        "\n",
        "params = []\n",
        "\n",
        "for alpha in tqdm(reg_powers):\n",
        "  # Lasso doesn't have an analytic solution. Instead it\n",
        "  # utilizes an iterative procedure, which for small\n",
        "  # alpha values may take a while to converge.\n",
        "  linear_model = Lasso(alpha=alpha, max_iter=1000000)\n",
        "  model = make_pipeline(\n",
        "    PolynomialFeatures(9, include_bias=False),\n",
        "    linear_model\n",
        "  )\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  params.append(\n",
        "      np.append(linear_model.coef_,\n",
        "                linear_model.intercept_)\n",
        "  )\n",
        "\n",
        "  train_mse.append(mean_squared_error(y_train, model.predict(X_train)))\n",
        "  test_mse.append(mean_squared_error(y_test, model.predict(X_test)))\n",
        "\n",
        "params = np.array(params)\n",
        "\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "\n",
        "plt.plot(1. / reg_powers, train_mse, label='train')\n",
        "plt.plot(1. / reg_powers, test_mse, label='test')\n",
        "plt.ylabel('MSE')\n",
        "plt.xlabel('inverse regularization power')\n",
        "plt.legend()\n",
        "plt.xscale('log')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(1. / reg_powers, np.abs(params));\n",
        "plt.xlabel(\"inverse regularization power\")\n",
        "plt.ylabel(\"parameter magnitude\")\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(1. / reg_powers, np.isclose(params, 0.).sum(axis=1));\n",
        "plt.xlabel(\"inverse regularization power\")\n",
        "plt.ylabel(\"# zero parameters\")\n",
        "plt.xscale('log')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NotxiAE2gJZH"
      },
      "source": [
        "# Bonus. What features are the most powerful?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7p_2HQldxXV"
      },
      "source": [
        "Let's see what features are most powerful for a reasonably performing model (e.g. 1/alpha = 100) **(2 points)**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKNFwkHldX77"
      },
      "outputs": [],
      "source": [
        "model = make_pipeline(\n",
        "  PolynomialFeatures(9, include_bias=False),\n",
        "  Lasso(alpha=0.01, max_iter=1000000)\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print('train mse = ', mean_squared_error(y_train, model.predict(X_train)))\n",
        "print('test mse = ', mean_squared_error(y_test, model.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqlQMxabgW9z"
      },
      "source": [
        "Some hints:\n",
        " - You can explore the feature names using `get_feature_names` method of the `PolynomialFeatures` class (plug the list of original feature names to get reasonable output).\n",
        " - `model.named_steps['polynomialfeatures']` to get the `PolynomialFeatures` preprocessor of our model.\n",
        " - `model.named_steps['lasso'].coef_` to get the parameters of the linear model\n",
        " - `np.argwhere` to find indices of non-zero elements of an array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKqrs_Hhe-sP"
      },
      "outputs": [],
      "source": [
        "<YOUR CODE>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}