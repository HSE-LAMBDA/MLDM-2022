{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HSE-LAMBDA/MLDM-2022/blob/master/02-linear-regression/LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1bJ2JqbrfLfj"
   },
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qDvcL1FjsZLY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zHoDZiwJsHlW"
   },
   "source": [
    "## y = kx + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8DvsrrgfPxC"
   },
   "source": [
    "Let's start with a toy 1D problem, where the true dependence is\n",
    "$$y=k\\cdot x+b+\\text{noise}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mKRgT8hyxRm_"
   },
   "outputs": [],
   "source": [
    "def linear_function(x):\n",
    "  return 0.33 * x + 8.3\n",
    "\n",
    "def gen_dataset(N, func, lims=(-1., 1.), noise_lvl=0.2):\n",
    "  x = np.random.uniform(*lims, size=N)\n",
    "  y = func(x) + noise_lvl * np.random.normal(size=x.shape)\n",
    "  return x[:,None], y\n",
    "\n",
    "X, y = gen_dataset(50, linear_function)\n",
    "x = np.linspace(-1, 1, 101)\n",
    "plt.plot(x, linear_function(x))\n",
    "plt.scatter(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bG5zspXRrYWL"
   },
   "source": [
    "### `LinearRegression` from `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "92YYaRU9e2jz"
   },
   "outputs": [],
   "source": [
    "# The following class implements the analytical solution for\n",
    "# linear regression with the MSE loss\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "x = np.linspace(-1, 1, 101)\n",
    "plt.plot(x, linear_function(x), label='true function')\n",
    "plt.scatter(X, y);\n",
    "plt.plot(x, model.predict(x[:,None]), label='prediction')\n",
    "plt.legend()\n",
    "\n",
    "print(model.coef_, model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4WGS--_XrSe9"
   },
   "source": [
    "### Sidenote: making contour plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ndqMFPx6oTpP"
   },
   "outputs": [],
   "source": [
    "### Sidenote: making contour plots (level maps)\n",
    "\n",
    "plt.contourf(\n",
    "    [[0., 1., 2.], # matrix of X\n",
    "     [0., 1., 2.],\n",
    "     [0., 1., 2.]],\n",
    "    [[ 0.,  0.,  0.], # matrix of Y\n",
    "     [10., 10., 10.],\n",
    "     [20., 20., 20.]],\n",
    "    [[-1., 0., -1.], # matrix of Z\n",
    "     [ 0., 1.,  0.],\n",
    "     [-1., 0., -1.]]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQs61KUCqV5G"
   },
   "outputs": [],
   "source": [
    "### 2D matrices of X and Y (as above) can be\n",
    "### created from 1D vectors using np.meshgrid:\n",
    "\n",
    "for i in np.meshgrid([0., 1., 2], [0., 10., 20.]):\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-PAHAotrkJV"
   },
   "source": [
    "### MSE as a function of model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HFQzedCIst18"
   },
   "source": [
    "Let's see what MSE looks like as a function of model parameters **(2 points)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MNT3O5soiWwb"
   },
   "outputs": [],
   "source": [
    "# Creating a grid of model parameter values:\n",
    "ww, bb = np.meshgrid(\n",
    "    np.linspace(-10., 10., 50),\n",
    "    np.linspace(-5., 15., 50)\n",
    ")\n",
    "\n",
    "# Your turn: calculate the map of MSE values on the grid defined above, i.e.\n",
    "# for each (w, b) in (ww, bb) calculate MSE for the model y = w * x + b.\n",
    "# Avoid using loops.\n",
    "MSE_map = <YOUR CODE>\n",
    "\n",
    "# Automatic checks\n",
    "assert MSE_map.shape == ww.shape\n",
    "for i in [0, -1]:\n",
    "  for j in [0, -1]:\n",
    "    assert np.isclose(\n",
    "        MSE_map[i, j],\n",
    "        ((ww[i, j] * X.ravel() + bb[i, j] - y)**2).mean()\n",
    "    ), f'assert failed for point {i, j}'\n",
    "\n",
    "# Plotting:\n",
    "plt.figure(figsize=(6, 5), dpi=100)\n",
    "plt.colorbar(plt.contourf(ww, bb, MSE_map, levels=30))\n",
    "plt.scatter(model.coef_, model.intercept_, marker='*', s=150, c='orange');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KA_a_UsFsYPR"
   },
   "source": [
    "## Polynomial fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gCSdUOg__u0n"
   },
   "source": [
    "Now let's take some arbitrary function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6X2splyeIVH"
   },
   "outputs": [],
   "source": [
    "def true_function(x):\n",
    "  return np.sin(3 * x + 0.8) + np.sin(1. / (x + 1.23))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPgBZq4l_0Ex"
   },
   "source": [
    "Obviously, we won't get a good fit with an ordinary linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KmSBngOE0JdP"
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "X, y = gen_dataset(25, true_function)\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "x = np.linspace(-1, 1, 101)\n",
    "plt.plot(x, true_function(x), label='true function')\n",
    "plt.scatter(X, y);\n",
    "plt.plot(x, model.predict(x[:,None]), label='prediction')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pXhZyytaANX2"
   },
   "source": [
    "### `PolynomialFeatures` and pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WbESfwRuAZD7"
   },
   "source": [
    "Even though our design matrix has only one column:\n",
    "$$X=\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_N\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "we can expand it with powers of $x$ to fit a polynomial:\n",
    "$$X'=\n",
    "\\begin{pmatrix}\n",
    "x_1 & (x_1)^2 & \\ldots & (x_1)^k \\\\\n",
    "x_2 & (x_2)^2 & \\ldots & (x_2)^k \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_N & (x_N)^2 & \\ldots & (x_N)^k\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "such that:\n",
    "\n",
    "$$\\frac{1}{N}\\left\\Vert X'\\cdot w - y\\right\\Vert^2\\to \\underset{w}{\\text{min}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wBX4E80aDfBv"
   },
   "source": [
    "This functionality is implemented in `sklearn.preprocessing.PolynomialFeatures`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tnvqh81sDeJb"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_expand = PolynomialFeatures(3)\n",
    "poly_expand.fit_transform(np.arange(5)[:,None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tD4hq4nVE0WO"
   },
   "source": [
    "One can combine `PolynomialFeatures` (and any other transformers) along with the model into a single pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jtNLILIP25-U"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXG8osw43CI-"
   },
   "outputs": [],
   "source": [
    "# The first parameter is the power of expansion. Try playing around with it.\n",
    "poly_expand = PolynomialFeatures(5, include_bias=False)\n",
    "linear_model = LinearRegression()\n",
    "model = make_pipeline(\n",
    "    poly_expand, linear_model\n",
    ")\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "x = np.linspace(-1, 1, 101)\n",
    "plt.plot(x, true_function(x), label='true function')\n",
    "plt.scatter(X, y);\n",
    "plt.plot(x, model.predict(x[:,None]), label='prediction')\n",
    "plt.ylim(y.min() - 0.5, y.max() + 0.5)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WiWLsor3c2hp"
   },
   "source": [
    "Now we want to plot 2D projections of MSE as a function of model parameters **(2 points)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UTXh8fe9-gXE"
   },
   "outputs": [],
   "source": [
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_xZFirLY3hHj"
   },
   "outputs": [],
   "source": [
    "# Combine the weights and the bias into a single parameter vector\n",
    "solution = np.concatenate([linear_model.coef_, [linear_model.intercept_]])\n",
    "\n",
    "# Calculate the power expansion of the features\n",
    "X_expanded = np.concatenate([\n",
    "    poly_expand.transform(X), np.ones(shape=(len(X), 1))\n",
    "], axis=1)\n",
    "\n",
    "# We'll plot a large matrix of plots, so let's create\n",
    "# a 16 by 16 inch canvas\n",
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "# We'll loop over all pairs of weights\n",
    "i_img = 0\n",
    "for dim1 in trange(len(solution)):\n",
    "  for dim2 in range(len(solution)):\n",
    "    i_img += 1\n",
    "    # Skip the diagonal\n",
    "    if dim1 == dim2: continue\n",
    "\n",
    "    # Create the grid of parameter values\n",
    "    ww1, ww2 = np.meshgrid(\n",
    "        np.linspace(solution[dim1] - 1000., solution[dim1] + 1000., 50),\n",
    "        np.linspace(solution[dim2] - 1000., solution[dim2] + 1000., 50),\n",
    "    )\n",
    "\n",
    "    # Your turn! To calculate the map of MSE values, let's first\n",
    "    # create `param_grid` - a 3D array of parameter values of the\n",
    "    # following shape: (len(solution), ww1.shape[0], ww1.shape[1])\n",
    "    #\n",
    "    # I.e. `param_grid[i, :, :]` should equal to:\n",
    "    #     `ww1` if `i` equals `dim1`;\n",
    "    #     `ww2` if `i` equals `dim2`;\n",
    "    #     `solution[i]` otherwise.\n",
    "    \n",
    "    param_grid = np.empty(shape= (len(solution),) + ww1.shape, dtype=float)\n",
    "    param_grid[:] = solution[:,None,None]\n",
    "    param_grid[dim1] = ww1\n",
    "    param_grid[dim2] = ww2\n",
    "\n",
    "    # Automatic checks\n",
    "    assert param_grid.shape == (len(solution),) + ww1.shape\n",
    "    assert np.allclose(param_grid[dim1], ww1)\n",
    "    assert np.allclose(param_grid[dim2], ww2)\n",
    "    assert all(\n",
    "        np.allclose(param_grid[i], solution[i])\n",
    "        for i in range(len(solution)) if i not in (dim1, dim2)\n",
    "    )\n",
    "\n",
    "    # Your turn! Now it's time to calculate the MSE map, i.e. for each grid\n",
    "    # element (i, j), you want `MSE_map[i, j]` to be equal to the MSE\n",
    "    # for the model defined by parameters `param_grid[:, i, j]`.\n",
    "    MSE_map = <YOUR CODE>\n",
    "\n",
    "    # Automatic checks\n",
    "    assert MSE_map.shape == ww1.shape\n",
    "    for i in [0, -1]:\n",
    "      for j in [0, -1]:\n",
    "        assert np.isclose(\n",
    "            ((X_expanded @ param_grid[:, i, j] - y)**2).mean(),\n",
    "            MSE_map[i, j]\n",
    "        ), f'Check failed for point {i, j}'\n",
    "\n",
    "    plt.subplot(len(solution), len(solution), i_img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.contourf(ww1, ww2, MSE_map, levels=10);\n",
    "    plt.scatter(solution[dim1], solution[dim2], marker='*', s=30, c='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jUbyj4uLMsMp"
   },
   "source": [
    "Note the relation between the amount of overfitting and correlation between parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J6Md8oRbHNJG"
   },
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aj-ZaSJCifkV"
   },
   "source": [
    "Let's look at MSE as a function of parameters:\n",
    "$$\\text{MSE}(w)=\\frac{1}{N}\\left\\Vert X'\\cdot w - y\\right\\Vert^2$$\n",
    "\n",
    "Instead of minimizing it analytically, we can use numeric optimization with gradient descent. I.e. do the following procedure iteratively:\n",
    "$$w\\leftarrow w-\\alpha\\cdot\\frac{\\partial\\text{MSE}(w)}{\\partial w},$$\n",
    "for some constant *learning rate* $\\alpha$.\n",
    "\n",
    "For the task below you'll need to derive the analytical formula for $\\frac{\\partial\\text{MSE}(w)}{\\partial w}$. **Note, that $w$ is a vector!** If not sure how to do it, check out the [matrix calculus cheat sheet](https://en.wikipedia.org/wiki/Matrix_calculus#Identities).\n",
    "\n",
    "When done **(2 points)**, play around with the power of the polynomial expansion, learning rate and the number of gradient descent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2AERh5jc-tBT"
   },
   "outputs": [],
   "source": [
    "# Initialize the model parameters with zeros\n",
    "w = np.zeros(dtype=float, shape=X_expanded.shape[1])\n",
    "\n",
    "loss_values = [] # a list to keep track of how the loss value changes\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Training loop\n",
    "for _ in trange(1000):\n",
    "  # Your turn: calculate the gradient of MSE with respect to w:\n",
    "  gradient = <YOUR CODE>\n",
    "\n",
    "  # Automatic checks\n",
    "  assert gradient.shape == w.shape\n",
    "  assert (\n",
    "      ((X_expanded @ w - y)**2).mean() > \n",
    "      ((X_expanded @ (w - 1.e-6 * gradient) - y)**2).mean()\n",
    "  )\n",
    "\n",
    "  # Gradient descent step\n",
    "  w -= learning_rate * gradient\n",
    "\n",
    "  # Calculate and record the new loss value\n",
    "  loss_values.append(\n",
    "      ((X_expanded @ w - y)**2).mean()\n",
    "  )\n",
    "\n",
    "# Plotting the evolution of loss values\n",
    "plt.plot(loss_values);\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plotting the solution\n",
    "x = np.linspace(-1, 1, 101)\n",
    "x_expanded = np.concatenate([\n",
    "    poly_expand.transform(x[:,None]),\n",
    "    np.ones(shape=(len(x), 1))\n",
    "], axis=1)\n",
    "plt.plot(x, true_function(x), label='true function')\n",
    "plt.scatter(X, y);\n",
    "plt.plot(x,\n",
    "         x_expanded @ w, label='prediction')\n",
    "plt.ylim(y.min() - 0.5, y.max() + 0.5)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LBenTP_0oDB-"
   },
   "source": [
    "Did you notice that numeric solution is less prone to overfitting? Some intuition for that can be found in this post: https://distill.pub/2017/momentum/ (though not explicitly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here comes the overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is going to happen if we increase the number of powers we use? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = gen_dataset(25, true_function)\n",
    "\n",
    "# Define the set of powers to be used generating the features\n",
    "set_of_powers =[ 1, 5, 15, 25 ]\n",
    "\n",
    "# Plotting the ground truth\n",
    "plt.figure(figsize=(15,6))\n",
    "x = np.linspace(-1, 1, 200)\n",
    "plt.plot(x, true_function(x), label='true function')\n",
    "plt.scatter(X, y,marker='*', s = 100);\n",
    "plt.ylim(-2, 2)\n",
    "\n",
    "# Adding plots for every single value in the set of powers using the pipeline\n",
    "for d in set_of_powers:\n",
    "    poly_expand = PolynomialFeatures(d, include_bias=False)\n",
    "    model = make_pipeline(poly_expand, LinearRegression())\n",
    "    model.fit(X, y)\n",
    "    plt.plot(x, model.predict(x[:,None]),linewidth=2, label=\"$d=%d$\" % d)\n",
    "plt.legend(loc = 1)\n",
    "plt.title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `sklearn` LinearRegression, it is possible to fit every single point, but it is not the objective we try to achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus. Custom LinearRegression class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on [OML](https://github.com/girafe-ai/ml-mipt) open materials.\n",
    "\n",
    "Let's wrap our previous Gradient Descent experiments into LR class, using `sklearn` standard interfaces to implement it **(3 points)**.\n",
    "You can find [the base classes](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.base) for all of the most common problems.\n",
    "So, we need to inherit base class and implement main stages of regression pipeline methods:\n",
    "* hyperparameter initialization - constructor\n",
    "* parameters training on known objects - fit method\n",
    "* target estimation for unknown objects - predict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRergessionSGD(BaseEstimator, RegressorMixin): # inherit base class \n",
    "    def __init__(self,\n",
    "                 batch_size: int=10,\n",
    "                 lr: float=1e-2,\n",
    "                 n_iters: int=10) -> None:\n",
    "        # You can read more about Annotations here: https://www.python.org/dev/peps/pep-3107/\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        \n",
    "        # We are going to use the history plotting the trajectory\n",
    "        self.w_history = [] \n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        # initialize the weights\n",
    "        w = <YOUR CODE> \n",
    "        \n",
    "        n_objects = len(X)\n",
    "        \n",
    "        for i in range(self.n_iters):\n",
    "            # Sample random indices of objects to be used during the step\n",
    "            sample_indices = <YOUR CODE>\n",
    "            \n",
    "            # Make the step\n",
    "            w -= <YOUR CODE> \n",
    "            self.w_history.append(w.copy())\n",
    "            \n",
    "        self.w = w\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Return the results of your model applied to X\n",
    "        return <YOUR CODE> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can have a look on the weights' trajectory through the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 2 \n",
    "n_objects = 300\n",
    "batch_size = 10\n",
    "num_steps = 15\n",
    "np.random.seed(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let it be the *true* weights vector\n",
    "w_true = np.random.normal(size=(n_features))\n",
    "X = np.random.uniform(-5, 5, (n_objects, n_features))\n",
    "\n",
    "# Let's make different scales of features\n",
    "X *= (np.arange(n_features) * 2 + 1)[np.newaxis, :] \n",
    "\n",
    "# Here you define the *true* target value\n",
    "Y = X.dot(w_true) + np.random.normal(0, 1, n_objects) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y)\n",
    "\n",
    "# Fit the model of our custom class\n",
    "own_lr = LinearRergessionSGD(batch_size=batch_size, n_iters=num_steps).fit(x_train, y_train)\n",
    "# Get the weights\n",
    "w_list = np.array(own_lr.w_history) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute level set\n",
    "A, B = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100)) \n",
    "\n",
    "levels = np.empty_like(A)\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        w_tmp = np.array([A[i, j], B[i, j]])\n",
    "        levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2))\n",
    "\n",
    "plt.figure(figsize=(13, 9))\n",
    "plt.title('LR weights GD trajectory')\n",
    "plt.xlabel('$w_1$')\n",
    "plt.ylabel('$w_2$')\n",
    "plt.xlim(w_list[:, 0].min() - 0.1, w_list[:, 0].max() + 0.1)\n",
    "plt.ylim(w_list[:, 1].min() - 0.1, w_list[:, 1].max() + 0.1)\n",
    "plt.gca().set_aspect('equal')\n",
    "\n",
    "# Visualize the level set\n",
    "CS = plt.contour(A, B, levels, levels=np.logspace(0, 1.8, num=10), cmap=plt.cm.rainbow_r)\n",
    "CB = plt.colorbar(CS, shrink=0.7)\n",
    "\n",
    "# Visualize the trajectory\n",
    "plt.scatter(w_true[0], w_true[1], c='r',marker = '*', s = 200)\n",
    "plt.scatter(w_list[:, 0], w_list[:, 1])\n",
    "plt.plot(w_list[:, 0], w_list[:, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we go again..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of `sklearn` Linear Regression we use our own SGD-based impementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = gen_dataset(25, true_function)\n",
    "set_of_powers =[ 1, 5, 15, 25 ]\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "x = np.linspace(-1, 1, 200)\n",
    "plt.plot(x, true_function(x), label='true function')\n",
    "plt.scatter(X, y,marker='*', s = 100);\n",
    "plt.ylim(-2, 2)\n",
    "\n",
    "for d in set_of_powers:\n",
    "    poly_expand = PolynomialFeatures(d, include_bias=True)\n",
    "    model = make_pipeline(poly_expand, LinearRergessionSGD(lr=1e-2,n_iters=2000))\n",
    "    model.fit(X, y)\n",
    "    plt.plot(x, model.predict(x[:,None]),linewidth=2, label=\"$d=%d$\" % d)\n",
    "    \n",
    "plt.legend(loc = 1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPAeTrEOAQZkGDiDCI9Y1Ay",
   "include_colab_link": true,
   "name": "LinearRegression.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
