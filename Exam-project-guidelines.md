# Exam project guidelines

As mentioned in the [course overview](https://github.com/HSE-LAMBDA/MLDM-2022/blob/main/01-intro/MLDM-2022-course-overview.pdf),
there are two options for the exam project: participating in an ML competition or implementing a technique from an advanced ML paper.
**Please submit your choice through [this form](https://docs.google.com/forms/d/e/1FAIpQLScNhxTYuDxaVw0tegRohyaT618LbfFwfopCn9lgIDdp3XXRqg/viewform)**.

There are 3 main checkpoints for the project:
|Checkpoint|Soft deadline|Hard deadline|
|--|--|--|
|Topic choice| Oct 1 | Nov 5|
|Intermediate status report|Nov 19|Dec 3|
|Project defense|Dec 17| Official exam date|

Meeting any of the soft deadlines gives you a +0.5 bonus for the exam score, each.
Missing any of the hard deadlines gives you a –0.5 penalty for the exam score, each.

Topic choice needs to be approved, so make sure you leave some time before the deadline for possible refinements.

Intermediate status report should cover at least 50% of the work that will be done by the project defense.
This intermediate report should consist of a github repository with your code, where your
**continuous progress should be clearly seen** (if you're participating in a competition within a team,
**contributions from all the members should be seen explicitly, via commits**).
Also, a brief supporting document should be provided (e.g. text or slides with comments) describing what
you have done and what you are planning to do by the end of the project.
The report will be graded in a binary form (passed/failed).

The project defense will be organized in a form of a public presentation of roughly 10-15 minutes per person.
A successfully passed intermediate status report is necessary to be admitted for the project defense.
You can find some guidelines below for the two project options.


## Option 1: competition

The main goal of a competition exam project is for you to demonstrate your skills in solving a real-world machine
learning problem. When choosing a competition, please avoid the ones tagged with the `Getting Started` or `Playground`
tags. Ideally, it should be an ongoing (or a finished competition with late submission allowed)
from the [`Monetary` category](https://www.kaggle.com/competitions?prestigeFilter=money). This option may be taken in
a team of up to 3 members. If so, the roles of all the members should be clearly stated and significant.
**Contributions from all the members should be seen explicitly, via commits in your project repository**
All members should participate in the project defense (public presentation).
A single competition may only be approved to at most 9 people (e.g. 9 individuals, 3 teams of 3, or anything in between).

You can consult the existing solutions, shared kernels, etc. but blind re-use of such materials is prohibited.
If you are basing (a part of) your solution on someone else’s work, be sure to make a proper reference to
that work, and also to have a sizable personal contribution on top of that.

Your work will be evaluated against the following criteria:

1. Skills demonstrated. Generally, by solving this task you want to show your skills
in **data preprocessing**, **feature engineering** and **feature selection**, **model development**,
**model selection** and **model quality assessment**. You should be able to justify your choices.
1. Presentation quality. Make sure that your audience understands:
    1. the problem you were trying to solve,
    1. what exactly you did, and
    1. what results you obtained

Getting high ranks on the leaderboard is not required, but it surely will be taken into account when grading if you do get it.


## Option 2: paper

Here you’re taking an advanced machine learning paper and implementing the method described in
it and/or reproducing (trying to validate) the experiments from that paper, if any.
Some paper suggestions are listed [here](https://docs.google.com/spreadsheets/d/1vnLl5nQCW4UaQ8CZwaw0nh6MNpdSPPqdg5mrWQihVt0/edit#gid=0).
You can suggest other papers as well. It may be OK to choose a paper related to your course work.
Please remember, that your choice needs to be approved (see above).

Your work will be evaluated against the following criteria:

1. Understanding of the paper. You want to demonstrate your skills in analyzing and understanding new material
related to research in machine learning. **Make sure that you can deliver the main ideas of the paper to a
person not familiar with it**.
1. Completeness and quality of the practical work done. It would not be ok to just run the code that was shared
by the authors of the original paper. **You need to implement the technique on your own**. If they did share the code - try to
make a comparison of your implementation vs theirs.
1. Presentation quality. Make sure that your audience understands:
    1. the problem you were trying to solve,
    1. what exactly you did, and 
    1. what results you obtained
